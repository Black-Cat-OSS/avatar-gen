name: Production Deploy Pipeline

on:
  # PR –≤ main (develop ‚Üí main) - –ø–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ë–ï–ó –¥–µ–ø–ª–æ—è
  pull_request:
    branches:
      - main
    types: [opened, synchronize, reopened]

  # Push –≤ main (–ø–æ—Å–ª–µ –º–µ—Ä–¥–∂–∞) - –ø–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ + –î–ï–ü–õ–û–ô
  push:
    branches:
      - main

  # –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –¥–ª—è —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
  workflow_dispatch:
    inputs:
      skip_tests:
        description: 'Skip tests (use only for hotfixes)'
        required: false
        default: false
        type: boolean
      skip_integration:
        description: 'Skip integration tests'
        required: false
        default: false
        type: boolean
      force_deploy:
        description: 'Force deploy (even on PR)'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  issues: write
  pull-requests: read

env:
  NODE_VERSION: '20'
  PNPM_VERSION: '10'

jobs:
  # ========================================
  # Stage 1: Unit & E2E Tests
  # ========================================
  # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –õ–∏–Ω—Ç–∏–Ω–≥ –ø—Ä–æ–ø—É—â–µ–Ω, —Ç.–∫. –∫–æ–¥ —É–∂–µ –ø—Ä–æ—à–µ–ª –ø—Ä–æ–≤–µ—Ä–∫—É
  # –≤ CI workflow –ø—Ä–∏ PR –≤ develop
  #
  # –≠—Ç–æ—Ç workflow –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –¥–ª—è:
  # - PR –≤ main: –ø–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ë–ï–ó –¥–µ–ø–ª–æ—è
  # - Push –≤ main: –ø–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ + –î–ï–ü–õ–û–ô
  test-backend:
    name: Test Backend (${{ matrix.test_name }})
    runs-on: ubuntu-latest
    if: github.event.inputs.skip_tests != 'true'
    strategy:
      fail-fast: false
      matrix:
        # –ü–æ–ª–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω –¥–µ–ø–ª–æ—è
        database: [sqlite, postgresql]
        storage: [local, s3]
        include:
          - database: sqlite
            storage: local
            test_name: 'SQLite + Local'
          - database: sqlite
            storage: s3
            test_name: 'SQLite + S3'
          - database: postgresql
            storage: local
            test_name: 'PostgreSQL + Local'
          - database: postgresql
            storage: s3
            test_name: 'PostgreSQL + S3'
    defaults:
      run:
        working-directory: backend

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: password
          POSTGRES_DB: avatar_gen_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Wait for PostgreSQL
        if: matrix.database == 'postgresql'
        run: |
          for i in {1..30}; do
            if pg_isready -h localhost -p 5432 -U postgres; then
              echo "PostgreSQL is ready"
              exit 0
            fi
            echo "Waiting for PostgreSQL... ($i/30)"
            sleep 2
          done
          echo "PostgreSQL failed to start"
          exit 1

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Get pnpm store directory
        id: pnpm-cache
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path)" >> $GITHUB_OUTPUT

      - name: Setup pnpm cache
        uses: actions/cache@v4
        with:
          path: ${{ steps.pnpm-cache.outputs.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Generate Prisma Client
        run: pnpm exec prisma generate

      - name: Create test configuration
        run: |
          # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
          cat > settings.test.matrix.yaml << EOF
          app:
            storage:
              type: '${{ matrix.storage }}'
              local:
                save_path: './storage/test-avatars'
              s3:
                endpoint: '${{ secrets.TEST_S3_ENDPOINT || 'https://test-s3-endpoint.com' }}'
                bucket: '${{ secrets.TEST_S3_BUCKET || 'avatar-gen-test' }}'
                access_key: '${{ secrets.TEST_S3_ACCESS_KEY || 'test-access-key' }}'
                secret_key: '${{ secrets.TEST_S3_SECRET_KEY || 'test-secret-key' }}'
                region: '${{ secrets.TEST_S3_REGION || 'us-east-1' }}'
                force_path_style: true
                connection:
                  maxRetries: 1
                  retryDelay: 100
            database:
              driver: '${{ matrix.database }}'
              connection:
                maxRetries: 1
                retryDelay: 100
              sqlite_params:
                url: 'file:./storage/test-database/database.test.sqlite'
              network:
                host: 'localhost'
                port: 5432
                database: 'avatar_gen_test'
                username: 'postgres'
                password: 'password'
                ssl: false
            logging:
              level: 'error'
              verbose: false
              pretty: false
          EOF

      - name: Run tests
        run: pnpm run test
        env:
          NODE_ENV: test
          TEST_MATRIX_CONFIG: ./settings.test.matrix.yaml

      - name: Run e2e tests
        run: pnpm run test:e2e
        env:
          NODE_ENV: test
          TEST_MATRIX_CONFIG: ./settings.test.matrix.yaml

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        if: always()
        with:
          files: ./backend/coverage/lcov.info
          flags: backend-${{ matrix.database }}-${{ matrix.storage }}
          name: backend-coverage-${{ matrix.database }}-${{ matrix.storage }}

  # ========================================
  # Stage 2: Build Frontend
  # ========================================
  build-frontend:
    name: Build Frontend
    runs-on: ubuntu-latest
    if: github.event.inputs.skip_tests != 'true'
    defaults:
      run:
        working-directory: frontend

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Get pnpm store directory
        id: pnpm-cache
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path)" >> $GITHUB_OUTPUT

      - name: Setup pnpm cache
        uses: actions/cache@v4
        with:
          path: ${{ steps.pnpm-cache.outputs.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Build
        run: pnpm run build

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: frontend-build
          path: frontend/dist
          retention-days: 1

  # ========================================
  # Stage 3: Docker Images Build (Parallel)
  # ========================================
  build-backend-image:
    name: Build Backend Image
    runs-on: ubuntu-latest
    needs: [test-backend]
    if: always() && (needs.test-backend.result == 'success' || needs.test-backend.result == 'skipped')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build backend image
        uses: docker/build-push-action@v6
        with:
          context: ./backend
          file: ./backend/docker/Dockerfile
          push: false
          tags: avatar-gen-backend:latest
          cache-from: type=gha,scope=backend
          cache-to: type=gha,mode=max,scope=backend

  build-frontend-image:
    name: Build Frontend Image
    runs-on: ubuntu-latest
    needs: [build-frontend]
    if: always() && (needs.build-frontend.result == 'success' || needs.build-frontend.result == 'skipped')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build frontend image
        uses: docker/build-push-action@v6
        with:
          context: ./frontend
          file: ./frontend/docker/Dockerfile
          push: false
          tags: avatar-gen-frontend:latest
          cache-from: type=gha,scope=frontend
          cache-to: type=gha,mode=max,scope=frontend

  build-gateway-image:
    name: Build Gateway Image
    runs-on: ubuntu-latest
    needs: [test-backend, build-frontend]
    if: always() && (needs.test-backend.result == 'success' || needs.test-backend.result == 'skipped') && (needs.build-frontend.result == 'success' || needs.build-frontend.result == 'skipped')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build gateway image
        uses: docker/build-push-action@v6
        with:
          context: ./gateway
          file: ./gateway/Dockerfile
          push: false
          tags: avatar-gen-gateway:latest
          cache-from: type=gha,scope=gateway
          cache-to: type=gha,mode=max,scope=gateway

  # ========================================
  # Stage 4: Integration Tests
  # ========================================
  integration-test:
    name: Integration Test (SQLite + Local)
    runs-on: ubuntu-latest
    needs: [build-backend-image, build-frontend-image, build-gateway-image]
    if: github.event.inputs.skip_integration != 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Create external network
        run: docker network create avatar-gen-external

      - name: Prepare integration test environment
        run: |
          echo "Preparing environment for SQLite + Local Storage test..."

          # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è storage
          mkdir -p backend/storage/database
          mkdir -p backend/storage/avatars

          # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª –ë–î –¥–ª—è SQLite –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∞–≤–∞
          touch backend/storage/database/database.sqlite
          chmod -R 777 backend/storage

          # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª—ã, —á—Ç–æ–±—ã Docker –Ω–µ —Å–æ–∑–¥–∞–ª –∏—Ö –∫–∞–∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
          touch backend/settings.local.yaml
          touch backend/settings.production.local.yaml
          touch backend/settings.production.yaml

          # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π settings.yaml –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ —Ç–µ—Å—Ç–∞
          cat > backend/settings.yaml << 'EOF'
          app:
            storage:
              type: 'local'
              local:
                save_path: './storage/avatars'
            server:
              host: '0.0.0.0'
              port: 3000
            database:
              driver: 'sqlite'
              connection:
                maxRetries: 3
                retryDelay: 2000
              sqlite_params:
                url: 'file:./storage/database/database.sqlite'
            logging:
              level: 'info'
              verbose: false
              pretty: true
          EOF

          echo "=== Created settings.yaml ==="
          cat backend/settings.yaml

          # –°–æ–∑–¥–∞–µ–º docker-compose.override.yml –¥–ª—è CI
          # –ö–†–ò–¢–ò–ß–ù–û: –û—Ç–∫–ª—é—á–∞–µ–º —Ä–µ—Å—Ç–∞—Ä—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–æ–≤!
          cat > docker/docker-compose.override.yml << 'EOF'
          services:
            avatar-backend:
              restart: "no"
              volumes:
                - ../backend/storage:/app/storage
                - ../backend/logs:/app/logs
                - ../backend/settings.yaml:/app/settings.yaml:ro
                - ../backend/settings.production.yaml:/app/settings.production.yaml:ro
            
            avatar-frontend:
              restart: "no"
            
            gateway:
              restart: "no"
          EOF

          echo "=== Docker Compose Override ==="
          cat docker/docker-compose.override.yml

          echo "=== Storage directory permissions ==="
          ls -la backend/storage/
          ls -la backend/storage/database/

      - name: Start services with Docker Compose
        run: |
          echo "üóÑÔ∏è  Starting services with SQLite + Local Storage..."
          docker compose -f docker/docker-compose.yml up --build -d

          echo "Waiting for services to be ready..."
          sleep 30

      - name: Check container status
        run: |
          echo "=== Docker Compose Services ==="
          docker compose -f docker/docker-compose.yml ps

      - name: Show backend logs for debugging
        if: always()
        run: |
          echo "=== Backend Container Logs ==="
          docker compose -f docker/docker-compose.yml logs avatar-backend
          echo "==========================================================="

      - name: Check backend health
        run: |
          echo "Checking backend health endpoint..."
          for i in {1..10}; do
            if curl -f http://localhost:3000/api/health; then
              echo "Backend is healthy!"
              exit 0
            fi
            echo "Attempt $i failed, retrying..."
            sleep 5
          done
          echo "Backend health check failed after 10 attempts"
          docker compose -f docker/docker-compose.yml logs avatar-backend
          exit 1

      - name: Check frontend health
        run: |
          echo "Checking frontend health endpoint..."
          curl -f http://localhost:80/health || (docker compose -f docker/docker-compose.yml logs avatar-frontend && exit 1)

      - name: Check gateway
        run: |
          echo "Checking gateway..."
          curl -f http://localhost:80/ || (docker compose -f docker/docker-compose.yml logs gateway && exit 1)

      - name: Collect logs on failure
        if: failure()
        run: |
          echo "=== Collecting logs from containers ==="
          mkdir -p test-artifacts/docker-logs

          # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –∏–∑ –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
          echo "Collecting docker compose logs..."
          docker compose -f docker/docker-compose.yml logs > test-artifacts/docker-logs/all-services.log 2>&1 || true
          docker compose -f docker/docker-compose.yml logs avatar-backend > test-artifacts/docker-logs/backend.log 2>&1 || true
          docker compose -f docker/docker-compose.yml logs avatar-frontend > test-artifacts/docker-logs/frontend.log 2>&1 || true
          docker compose -f docker/docker-compose.yml logs gateway > test-artifacts/docker-logs/gateway.log 2>&1 || true

          # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
          docker compose -f docker/docker-compose.yml ps > test-artifacts/container-status.txt 2>&1 || true
          docker ps -a > test-artifacts/docker-ps.txt 2>&1 || true

          echo "=== Logs collected to test-artifacts/ ==="
          ls -la test-artifacts/

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-logs-sqlite-local-${{ github.run_id }}
          path: test-artifacts/
          retention-days: 7
          if-no-files-found: warn

      - name: Create GitHub Issue with logs on failure
        if: failure() && github.event_name == 'push'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const testName = 'SQLite + Local';
            const database = 'sqlite';
            const storage = 'local';
            const branch = '${{ github.ref_name }}';
            const sha = '${{ github.sha }}';
            const runUrl = '${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}';
            const commitUrl = '${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}';
            const actor = '${{ github.actor }}';
            const workflow = '${{ github.workflow }}';

            // –ß–∏—Ç–∞–µ–º –ª–æ–≥–∏ –∏–∑ test-artifacts
            let backendLogs = '';
            let containerStatus = '';

            try {
              backendLogs = fs.readFileSync('test-artifacts/docker-logs/backend.log', 'utf8');
            } catch (e) {
              backendLogs = `Could not read backend logs: ${e.message}`;
            }

            try {
              containerStatus = fs.readFileSync('test-artifacts/container-status.txt', 'utf8');
            } catch (e) {
              containerStatus = `Could not read container status: ${e.message}`;
            }

            // –°–æ–∑–¥–∞—ë–º Gist —Å –ø–æ–ª–Ω—ã–º–∏ –ª–æ–≥–∞–º–∏ (permanent storage)
            let gistUrl = '';
            try {
              const gist = await github.rest.gists.create({
                public: false,
                description: `Integration Test Logs - ${testName} - ${sha.substring(0, 7)}`,
                files: {
                  'backend-full.log': { 
                    content: backendLogs.substring(0, 1000000) || 'No logs available'  // Gist limit 1MB per file
                  },
                  'container-status.txt': { 
                    content: containerStatus || 'No status available' 
                  }
                }
              });
              gistUrl = gist.data.html_url;
              console.log(`Created Gist: ${gistUrl}`);
            } catch (e) {
              console.log(`Could not create gist: ${e.message}`);
            }

            // –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 —Å—Ç—Ä–æ–∫ –¥–ª—è issue body
            const logLines = backendLogs.split('\n');
            const lastLines = logLines.slice(-100).join('\n');

            const body = `## Integration Test Failure Report

            **Test Variant:** ${testName}
            **Database:** ${database}
            **Storage:** ${storage}
            **Branch:** \`${branch}\`
            **Commit:** ${sha.substring(0, 7)}
            **Triggered by:** @${actor}

            ### Environment
            - **DATABASE_PROVIDER:** \`${database}\`
            - **STORAGE_TYPE:** \`${storage}\`
            - **NODE_ENV:** \`production\`

            ### Quick Links
            - [üîß Workflow Run](${runUrl})
            - [üìù Commit](${commitUrl})
            ${gistUrl ? `- [üìã Full Logs (Gist - permanent)](${gistUrl})` : ''}
            - [üì¶ Artifacts (expires in 7 days)](${runUrl}#artifacts)

            ### Container Status
            <details>
            <summary>Click to expand</summary>

            \`\`\`
            ${containerStatus}
            \`\`\`
            </details>

            ### Backend Logs (last 100 lines)
            <details>
            <summary>Click to expand</summary>

            \`\`\`
            ${lastLines}
            \`\`\`
            </details>

            ### Next Steps
            1. Check [full logs in Gist](${gistUrl || runUrl}) (permanent) or [Artifacts](${runUrl}#artifacts) (7 days)
            2. Review backend container startup sequence
            3. Verify Prisma schema generation
            4. Check database connectivity (\`${database}\`)
            5. Verify storage configuration (\`${storage}\`)

            ---
            *Auto-created by GitHub Actions - Workflow: \`${workflow}\`*`;

            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `CI Failure: Integration Test ${testName}`,
              body: body,
              labels: ['ci-failure', 'integration-test', 'automated', `db:${database}`, `storage:${storage}`]
            });

            console.log(`Created issue #${issue.data.number}`);

      - name: Stop services
        if: always()
        run: docker compose -f docker/docker-compose.yml down --volumes

  # ========================================
  # Stage 5: Production Deployment
  # ========================================
  deploy:
    name: Deploy to Production Server
    runs-on: ubuntu-latest
    needs: [integration-test]
    # –î–µ–ø–ª–æ–π —Ç–æ–ª—å–∫–æ –ø—Ä–∏:
    # - push –≤ main (–ø–æ—Å–ª–µ –º–µ—Ä–¥–∂–∞ PR)
    # - —Ä—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ —Å force_deploy=true
    # –ù–ï –¥–µ–ø–ª–æ–∏–º –ø—Ä–∏ PR –≤ main (—Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç–∏—Ä—É–µ–º)
    if: |
      always() &&
      (needs.integration-test.result == 'success' || needs.integration-test.result == 'skipped') &&
      (github.event_name == 'push' || github.event.inputs.force_deploy == 'true')
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -p ${{ secrets.SSH_PORT }} ${{ secrets.SSH_HOST }} >> ~/.ssh/known_hosts

      - name: Deploy to server
        env:
          SSH_HOST: ${{ secrets.SSH_HOST }}
          SSH_PORT: ${{ secrets.SSH_PORT }}
          SSH_USERNAME: ${{ secrets.SSH_USERNAME }}
          APP_PATH: ${{ secrets.APP_PATH }}
        run: |
          ssh -p $SSH_PORT $SSH_USERNAME@$SSH_HOST << 'ENDSSH'
            set -e
            cd ${{ secrets.APP_PATH }}
            
            echo "==================================="
            echo "  Production Deployment Started"
            echo "==================================="
            
            echo "üì• Pulling latest changes..."
            git fetch origin
            git reset --hard origin/main
            
            echo "üèóÔ∏è  Building Docker images..."
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–Ω–µ—à–Ω—é—é PostgreSQL (–±–µ–∑ –ø—Ä–æ—Ñ–∏–ª—è postgresql)
            docker compose -f docker/docker-compose.yml build --no-cache
            
            echo "üõë Stopping old containers..."
            docker compose -f docker/docker-compose.yml down
            
            echo "üöÄ Starting new containers..."
            # –ó–∞–ø—É—Å–∫ —Å –≤–Ω–µ—à–Ω–µ–π PostgreSQL (DATABASE_URL –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ .env –∏–ª–∏ secrets)
            docker compose -f docker/docker-compose.yml up -d
            
            echo "üßπ Cleaning up old images..."
            docker image prune -f
            
            echo "‚úÖ Deployment completed successfully!"
          ENDSSH

      - name: Verify deployment
        env:
          SSH_HOST: ${{ secrets.SSH_HOST }}
          SSH_PORT: ${{ secrets.SSH_PORT }}
          SSH_USERNAME: ${{ secrets.SSH_USERNAME }}
          APP_PATH: ${{ secrets.APP_PATH }}
        run: |
          ssh -p $SSH_PORT $SSH_USERNAME@$SSH_HOST << 'ENDSSH'
            cd ${{ secrets.APP_PATH }}
            
            echo "Checking container status..."
            docker compose -f docker/docker-compose.yml ps
            
            echo "Checking backend health..."
            sleep 10
            curl -f http://localhost:3000/api/health || exit 1
            
            echo "All health checks passed!"
          ENDSSH

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/id_rsa
