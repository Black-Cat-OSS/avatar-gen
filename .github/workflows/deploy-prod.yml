name: Production Deploy Pipeline

on:
  # PR –≤ main (develop ‚Üí main) - –ø–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ë–ï–ó –¥–µ–ø–ª–æ—è
  pull_request:
    branches:
      - main
    types: [opened, synchronize, reopened]

  # Push –≤ main (–ø–æ—Å–ª–µ –º–µ—Ä–¥–∂–∞) - –ø–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ + –î–ï–ü–õ–û–ô
  push:
    branches:
      - main

  # –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –¥–ª—è —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
  workflow_dispatch:
    inputs:
      skip_tests:
        description: 'Skip tests (use only for hotfixes)'
        required: false
        default: false
        type: boolean
      skip_integration:
        description: 'Skip integration tests'
        required: false
        default: false
        type: boolean
      force_deploy:
        description: 'Force deploy (even on PR)'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  issues: write
  pull-requests: read

env:
  NODE_VERSION: '20'
  PNPM_VERSION: '10'

jobs:
  # ========================================
  # Stage 1: Unit & E2E Tests
  # ========================================
  # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –õ–∏–Ω—Ç–∏–Ω–≥ –ø—Ä–æ–ø—É—â–µ–Ω, —Ç.–∫. –∫–æ–¥ —É–∂–µ –ø—Ä–æ—à–µ–ª –ø—Ä–æ–≤–µ—Ä–∫—É
  # –≤ CI workflow –ø—Ä–∏ PR –≤ develop
  #
  # –≠—Ç–æ—Ç workflow –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –¥–ª—è:
  # - PR –≤ main: –ø–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ë–ï–ó –¥–µ–ø–ª–æ—è
  # - Push –≤ main: –ø–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ + –î–ï–ü–õ–û–ô
  test-backend:
    name: Test Backend (${{ matrix.test_name }})
    runs-on: ubuntu-latest
    if: github.event.inputs.skip_tests != 'true'
    strategy:
      fail-fast: false
      matrix:
        # –ü–æ–ª–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω –¥–µ–ø–ª–æ—è
        database: [sqlite, postgresql]
        storage: [local, s3]
        include:
          - database: sqlite
            storage: local
            test_name: 'SQLite + Local'
          - database: sqlite
            storage: s3
            test_name: 'SQLite + S3'
          - database: postgresql
            storage: local
            test_name: 'PostgreSQL + Local'
          - database: postgresql
            storage: s3
            test_name: 'PostgreSQL + S3'
    defaults:
      run:
        working-directory: backend

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: password
          POSTGRES_DB: avatar_gen_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Wait for PostgreSQL
        if: matrix.database == 'postgresql'
        run: |
          for i in {1..30}; do
            if pg_isready -h localhost -p 5432 -U postgres; then
              echo "PostgreSQL is ready"
              exit 0
            fi
            echo "Waiting for PostgreSQL... ($i/30)"
            sleep 2
          done
          echo "PostgreSQL failed to start"
          exit 1

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Get pnpm store directory
        id: pnpm-cache
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path)" >> $GITHUB_OUTPUT

      - name: Setup pnpm cache
        uses: actions/cache@v4
        with:
          path: ${{ steps.pnpm-cache.outputs.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Generate Prisma Client
        run: pnpm exec prisma generate

      - name: Create test configuration
        run: |
          # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
          cat > settings.test.matrix.yaml << EOF
          app:
            storage:
              type: '${{ matrix.storage }}'
              local:
                save_path: './storage/test-avatars'
              s3:
                endpoint: '${{ secrets.TEST_S3_ENDPOINT || 'https://test-s3-endpoint.com' }}'
                bucket: '${{ secrets.TEST_S3_BUCKET || 'avatar-gen-test' }}'
                access_key: '${{ secrets.TEST_S3_ACCESS_KEY || 'test-access-key' }}'
                secret_key: '${{ secrets.TEST_S3_SECRET_KEY || 'test-secret-key' }}'
                region: '${{ secrets.TEST_S3_REGION || 'us-east-1' }}'
                force_path_style: true
                connection:
                  maxRetries: 1
                  retryDelay: 100
            database:
              driver: '${{ matrix.database }}'
              connection:
                maxRetries: 1
                retryDelay: 100
              sqlite_params:
                url: 'file:./storage/test-database/database.test.sqlite'
              network:
                host: 'localhost'
                port: 5432
                database: 'avatar_gen_test'
                username: 'postgres'
                password: 'password'
                ssl: false
            logging:
              level: 'error'
              verbose: false
              pretty: false
          EOF

      - name: Run tests
        run: pnpm run test
        env:
          NODE_ENV: test
          TEST_MATRIX_CONFIG: ./settings.test.matrix.yaml

      - name: Run e2e tests
        run: pnpm run test:e2e
        env:
          NODE_ENV: test
          TEST_MATRIX_CONFIG: ./settings.test.matrix.yaml

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        if: always()
        with:
          files: ./backend/coverage/lcov.info
          flags: backend-${{ matrix.database }}-${{ matrix.storage }}
          name: backend-coverage-${{ matrix.database }}-${{ matrix.storage }}

  # ========================================
  # Stage 2: Build Frontend
  # ========================================
  build-frontend:
    name: Build Frontend
    runs-on: ubuntu-latest
    if: github.event.inputs.skip_tests != 'true'
    defaults:
      run:
        working-directory: frontend

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Get pnpm store directory
        id: pnpm-cache
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path)" >> $GITHUB_OUTPUT

      - name: Setup pnpm cache
        uses: actions/cache@v4
        with:
          path: ${{ steps.pnpm-cache.outputs.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Build
        run: pnpm run build

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: frontend-build
          path: frontend/dist
          retention-days: 1

  # ========================================
  # Stage 3: Docker Images Build (Parallel)
  # ========================================
  build-backend-image:
    name: Build Backend Image
    runs-on: ubuntu-latest
    needs: [test-backend]
    if: always() && (needs.test-backend.result == 'success' || needs.test-backend.result == 'skipped')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build backend image
        uses: docker/build-push-action@v6
        with:
          context: ./backend
          file: ./backend/docker/Dockerfile
          push: false
          tags: avatar-gen-backend:latest
          cache-from: type=gha,scope=backend
          cache-to: type=gha,mode=max,scope=backend

  build-frontend-image:
    name: Build Frontend Image
    runs-on: ubuntu-latest
    needs: [build-frontend]
    if: always() && (needs.build-frontend.result == 'success' || needs.build-frontend.result == 'skipped')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build frontend image
        uses: docker/build-push-action@v6
        with:
          context: ./frontend
          file: ./frontend/docker/Dockerfile
          push: false
          tags: avatar-gen-frontend:latest
          cache-from: type=gha,scope=frontend
          cache-to: type=gha,mode=max,scope=frontend

  build-gateway-image:
    name: Build Gateway Image
    runs-on: ubuntu-latest
    needs: [test-backend, build-frontend]
    if: always() && (needs.test-backend.result == 'success' || needs.test-backend.result == 'skipped') && (needs.build-frontend.result == 'success' || needs.build-frontend.result == 'skipped')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build gateway image
        uses: docker/build-push-action@v6
        with:
          context: ./gateway
          file: ./gateway/Dockerfile
          push: false
          tags: avatar-gen-gateway:latest
          cache-from: type=gha,scope=gateway
          cache-to: type=gha,mode=max,scope=gateway

  # ========================================
  # Stage 4: Integration Tests
  # ========================================
  integration-test:
    name: Integration Test (${{ matrix.test_name }})
    runs-on: ubuntu-latest
    needs: [build-backend-image, build-frontend-image, build-gateway-image]
    if: github.event.inputs.skip_integration != 'true'

    strategy:
      fail-fast: false
      matrix:
        database: [sqlite, postgresql]
        storage: [local, s3]
        include:
          - database: sqlite
            storage: local
            test_name: 'SQLite + Local'
            db_url: 'file:./storage/database/database.sqlite'
          - database: sqlite
            storage: s3
            test_name: 'SQLite + S3'
            db_url: 'file:./storage/database/database.sqlite'
          - database: postgresql
            storage: local
            test_name: 'PostgreSQL + Local'
            db_url: 'postgresql://postgres:password@postgres:5432/avatar_gen'
          - database: postgresql
            storage: s3
            test_name: 'PostgreSQL + S3'
            db_url: 'postgresql://postgres:password@postgres:5432/avatar_gen'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Create external network
        run: docker network create avatar-gen-external

      - name: Prepare integration test environment (${{ matrix.test_name }})
        run: |
          echo "Preparing environment for ${{ matrix.test_name }} test..."

          # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è storage
          mkdir -p backend/storage/database
          mkdir -p backend/storage/avatars

          # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª –ë–î –¥–ª—è SQLite –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∞–≤–∞
          touch backend/storage/database/database.sqlite
          chmod -R 777 backend/storage

          # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª—ã, —á—Ç–æ–±—ã Docker –Ω–µ —Å–æ–∑–¥–∞–ª –∏—Ö –∫–∞–∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
          # –ö–†–ò–¢–ò–ß–ù–û: –ü—É—Å—Ç–æ–π settings.production.yaml –Ω–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç settings.yaml!
          touch backend/settings.local.yaml
          touch backend/settings.production.local.yaml

          # –°–æ–∑–¥–∞—ë–º –ü–£–°–¢–û–ô settings.production.yaml, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å
          # –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å S3 –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
          cat > backend/settings.production.yaml << 'EOF'
          # –ü—É—Å—Ç–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è CI —Ç–µ—Å—Ç–æ–≤
          # –í—Å–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–µ—Ä—É—Ç—Å—è –∏–∑ settings.yaml —Å ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
          EOF

          # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π settings.yaml —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
          cat > backend/settings.yaml << 'EOF'
          app:
            storage:
              type: '${STORAGE_TYPE:-local}'
              local:
                save_path: './storage/avatars'
              s3:
                endpoint: '${S3_ENDPOINT:-http://localhost:9000}'
                bucket: '${S3_BUCKET:-avatars}'
                access_key: '${S3_ACCESS_KEY:-minioadmin}'
                secret_key: '${S3_SECRET_KEY:-minioadmin}'
                region: '${S3_REGION:-us-east-1}'
                force_path_style: true
                connection:
                  maxRetries: 3
                  retryDelay: 2000
            server:
              host: '0.0.0.0'
              port: 3000
            database:
              driver: '${DATABASE_PROVIDER:-sqlite}'
              connection:
                maxRetries: 3
                retryDelay: 2000
              sqlite_params:
                url: '${DATABASE_URL:-file:./storage/database/database.sqlite}'
              network:
                host: '${DB_HOST:-localhost}'
                port: 5432
                database: '${DB_NAME:-avatar_gen_test}'
                username: '${DB_USER:-postgres}'
                password: '${DB_PASSWORD:-password}'
                ssl: false
            logging:
              level: 'info'
              verbose: false
              pretty: true
          EOF

          echo "=== Created settings.yaml ==="
          cat backend/settings.yaml

          echo "=== Created empty settings.production.yaml ==="
          cat backend/settings.production.yaml

          # –°–æ–∑–¥–∞–µ–º docker-compose.override.yml –¥–ª—è CI
          # –ö–†–ò–¢–ò–ß–ù–û: –û—Ç–∫–ª—é—á–∞–µ–º —Ä–µ—Å—Ç–∞—Ä—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–æ–≤!
          cat > docker/docker-compose.override.yml << 'EOF'
          services:
            avatar-backend:
              restart: "no"
              volumes:
                - ../backend/storage:/app/storage
                - ../backend/logs:/app/logs
                - ../backend/settings.yaml:/app/settings.yaml:ro
                - ../backend/settings.production.yaml:/app/settings.production.yaml:ro
            
            postgres:
              restart: "no"
            
            avatar-frontend:
              restart: "no"
            
            gateway:
              restart: "no"
          EOF

          echo "=== Docker Compose Override ==="
          cat docker/docker-compose.override.yml

          echo "=== Storage directory permissions ==="
          ls -la backend/storage/
          ls -la backend/storage/database/

      - name: Start services with Docker Compose (${{ matrix.test_name }})
        run: |
          # –°–æ–∑–¥–∞—ë–º .env —Ñ–∞–π–ª –¥–ª—è docker-compose –≤ –ö–û–†–ù–ï (–≤–∞–∂–Ω–æ!)
          # Docker Compose –∏—â–µ—Ç .env –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –∞ –ù–ï –≤ docker/
          cat > .env << EOF
          DATABASE_URL=${{ matrix.db_url }}
          DATABASE_PROVIDER=${{ matrix.database }}
          STORAGE_TYPE=${{ matrix.storage }}
          NODE_ENV=production
          S3_ENDPOINT=${{ secrets.PROD_S3_ENDPOINT || secrets.TEST_S3_ENDPOINT || 'http://localhost:9000' }}
          S3_BUCKET=${{ secrets.PROD_S3_BUCKET || secrets.TEST_S3_BUCKET || 'avatars' }}
          S3_ACCESS_KEY=${{ secrets.PROD_S3_ACCESS_KEY || secrets.TEST_S3_ACCESS_KEY || 'minioadmin' }}
          S3_SECRET_KEY=${{ secrets.PROD_S3_SECRET_KEY || secrets.TEST_S3_SECRET_KEY || 'minioadmin' }}
          S3_REGION=${{ secrets.PROD_S3_REGION || secrets.TEST_S3_REGION || 'us-east-1' }}
          EOF

          echo "=== Created .env file (root) ==="
          cat .env | grep -v "SECRET_KEY"  # –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–µ–∫—Ä–µ—Ç—ã –≤ –ª–æ–≥–∞—Ö
          echo "S3_SECRET_KEY=***"
          echo "================================="

          # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å—ã
          # –î–ª—è PostgreSQL –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ñ–∏–ª—å --profile postgresql
          if [ "${{ matrix.database }}" = "postgresql" ]; then
            echo "üêò Starting services with PostgreSQL container..."
            docker compose --profile postgresql -f docker/docker-compose.yml up --build -d
          else
            echo "üóÑÔ∏è  Starting services with SQLite..."
            docker compose -f docker/docker-compose.yml up --build -d
          fi

          echo "Waiting for services to be ready..."
          sleep 45

      - name: Check container status
        run: |
          echo "=== Docker Compose Services (${{ matrix.test_name }}) ==="
          docker compose -f docker/docker-compose.yml ps

      - name: Show backend logs for debugging (${{ matrix.test_name }})
        if: always()
        run: |
          echo "=== Backend Container Logs (${{ matrix.test_name }}) ==="
          docker compose -f docker/docker-compose.yml logs avatar-backend
          echo "==========================================================="

          echo "=== Backend Container Environment ==="
          docker compose -f docker/docker-compose.yml exec -T avatar-backend env | grep -E "(DATABASE|STORAGE|NODE_ENV|CONFIG)" || echo "Container not running or exec failed"
          echo "======================================"

          echo "=== Generated Prisma Schema Check ==="
          docker compose -f docker/docker-compose.yml exec -T avatar-backend cat /app/prisma/schema.prisma | head -15 || echo "Cannot read schema"
          echo "======================================"

          if [ "${{ matrix.database }}" = "postgresql" ]; then
            echo "=== PostgreSQL Container Logs ==="
            docker compose -f docker/docker-compose.yml logs postgres
            echo "================================="
          fi

      - name: Check backend health
        run: |
          echo "Checking backend health endpoint..."
          for i in {1..10}; do
            if curl -f http://localhost:3000/api/health; then
              echo "Backend is healthy!"
              exit 0
            fi
            echo "Attempt $i failed, retrying..."
            sleep 5
          done
          echo "Backend health check failed after 10 attempts"
          docker compose -f docker/docker-compose.yml logs avatar-backend
          exit 1

      - name: Check frontend health
        run: |
          echo "Checking frontend health endpoint..."
          curl -f http://localhost:80/health || (docker compose -f docker/docker-compose.yml logs avatar-frontend && exit 1)

      - name: Check gateway
        run: |
          echo "Checking gateway..."
          curl -f http://localhost:80/ || (docker compose -f docker/docker-compose.yml logs gateway && exit 1)

      - name: Collect logs on failure
        if: failure()
        run: |
          echo "=== Collecting logs from containers ==="
          mkdir -p test-artifacts/logs
          mkdir -p test-artifacts/docker-logs

          # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –∏–∑ –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
          echo "Collecting docker compose logs..."
          docker compose -f docker/docker-compose.yml logs > test-artifacts/docker-logs/all-services.log 2>&1 || true
          docker compose -f docker/docker-compose.yml logs avatar-backend > test-artifacts/docker-logs/backend.log 2>&1 || true
          docker compose -f docker/docker-compose.yml logs avatar-frontend > test-artifacts/docker-logs/frontend.log 2>&1 || true
          docker compose -f docker/docker-compose.yml logs gateway > test-artifacts/docker-logs/gateway.log 2>&1 || true

          if [ "${{ matrix.database }}" = "postgresql" ]; then
            docker compose -f docker/docker-compose.yml logs postgres > test-artifacts/docker-logs/postgres.log 2>&1 || true
          fi

          # –ö–æ–ø–∏—Ä—É–µ–º –ª–æ–≥–∏ –∏–∑ volume (–µ—Å–ª–∏ –µ—Å—Ç—å)
          echo "Copying logs from backend volume..."
          docker compose -f docker/docker-compose.yml exec -T avatar-backend ls -la /app/logs 2>&1 | tee test-artifacts/logs/volume-listing.txt || true
          docker cp avatar-gen-backend:/app/logs test-artifacts/backend-app-logs 2>&1 || true

          # –ö–æ–ø–∏—Ä—É–µ–º .env —Ñ–∞–π–ª –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (–±–µ–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤!)
          cat .env | grep -v "SECRET" > test-artifacts/root-env-file.txt 2>&1 || true

          # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
          docker compose -f docker/docker-compose.yml ps > test-artifacts/container-status.txt 2>&1 || true
          docker ps -a > test-artifacts/docker-ps.txt 2>&1 || true

          # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ—Ç–∏
          docker network ls > test-artifacts/networks.txt 2>&1 || true

          echo "=== Logs collected to test-artifacts/ ==="
          ls -la test-artifacts/

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-logs-${{ matrix.test_name }}-${{ github.run_id }}
          path: test-artifacts/
          retention-days: 7
          if-no-files-found: warn

      - name: Create GitHub Issue with logs on failure
        if: failure() && github.event_name == 'push'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const testName = '${{ matrix.test_name }}';
            const database = '${{ matrix.database }}';
            const storage = '${{ matrix.storage }}';
            const dbUrl = '${{ matrix.db_url }}';
            const branch = '${{ github.ref_name }}';
            const sha = '${{ github.sha }}';
            const runUrl = '${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}';
            const commitUrl = '${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}';
            const actor = '${{ github.actor }}';
            const workflow = '${{ github.workflow }}';

            // –ß–∏—Ç–∞–µ–º –ª–æ–≥–∏ –∏–∑ test-artifacts
            let backendLogs = '';
            let containerStatus = '';
            let envVars = '';

            try {
              backendLogs = fs.readFileSync('test-artifacts/docker-logs/backend.log', 'utf8');
            } catch (e) {
              backendLogs = `Could not read backend logs: ${e.message}`;
            }

            try {
              containerStatus = fs.readFileSync('test-artifacts/container-status.txt', 'utf8');
            } catch (e) {
              containerStatus = `Could not read container status: ${e.message}`;
            }

            try {
              envVars = fs.readFileSync('test-artifacts/root-env-file.txt', 'utf8');
            } catch (e) {
              envVars = `Could not read env vars: ${e.message}`;
            }

            // –°–æ–∑–¥–∞—ë–º Gist —Å –ø–æ–ª–Ω—ã–º–∏ –ª–æ–≥–∞–º–∏ (permanent storage)
            let gistUrl = '';
            try {
              const gist = await github.rest.gists.create({
                public: false,
                description: `Integration Test Logs - ${testName} - ${sha.substring(0, 7)}`,
                files: {
                  'backend-full.log': { 
                    content: backendLogs.substring(0, 1000000) || 'No logs available'  // Gist limit 1MB per file
                  },
                  'container-status.txt': { 
                    content: containerStatus || 'No status available' 
                  },
                  'environment-vars.txt': { 
                    content: envVars || 'No env vars available' 
                  }
                }
              });
              gistUrl = gist.data.html_url;
              console.log(`Created Gist: ${gistUrl}`);
            } catch (e) {
              console.log(`Could not create gist: ${e.message}`);
            }

            // –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 —Å—Ç—Ä–æ–∫ –¥–ª—è issue body
            const logLines = backendLogs.split('\n');
            const lastLines = logLines.slice(-100).join('\n');

            const body = `## Integration Test Failure Report

            **Test Variant:** ${testName}
            **Database:** ${database}
            **Storage:** ${storage}
            **Branch:** \`${branch}\`
            **Commit:** ${sha.substring(0, 7)}
            **Triggered by:** @${actor}

            ### Environment
            - **DATABASE_URL:** \`${dbUrl}\`
            - **DATABASE_PROVIDER:** \`${database}\`
            - **STORAGE_TYPE:** \`${storage}\`
            - **NODE_ENV:** \`production\`

            ### Quick Links
            - [üîß Workflow Run](${runUrl})
            - [üìù Commit](${commitUrl})
            ${gistUrl ? `- [üìã Full Logs (Gist - permanent)](${gistUrl})` : ''}
            - [üì¶ Artifacts (expires in 7 days)](${runUrl}#artifacts)

            ### Container Status
            <details>
            <summary>Click to expand</summary>

            \`\`\`
            ${containerStatus}
            \`\`\`
            </details>

            ### Backend Logs (last 100 lines)
            <details>
            <summary>Click to expand</summary>

            \`\`\`
            ${lastLines}
            \`\`\`
            </details>

            ### Next Steps
            1. Check [full logs in Gist](${gistUrl || runUrl}) (permanent) or [Artifacts](${runUrl}#artifacts) (7 days)
            2. Review backend container startup sequence
            3. Verify Prisma schema generation
            4. Check database connectivity (\`${database}\`)
            5. Verify storage configuration (\`${storage}\`)

            ---
            *Auto-created by GitHub Actions - Workflow: \`${workflow}\`*`;

            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `CI Failure: Integration Test ${testName}`,
              body: body,
              labels: ['ci-failure', 'integration-test', 'automated', `db:${database}`, `storage:${storage}`]
            });

            console.log(`Created issue #${issue.data.number}`);

      - name: Stop services
        if: always()
        run: docker compose -f docker/docker-compose.yml down --volumes

  # ========================================
  # Stage 5: Production Deployment
  # ========================================
  deploy:
    name: Deploy to Production Server
    runs-on: ubuntu-latest
    needs: [integration-test]
    # –î–µ–ø–ª–æ–π —Ç–æ–ª—å–∫–æ –ø—Ä–∏:
    # - push –≤ main (–ø–æ—Å–ª–µ –º–µ—Ä–¥–∂–∞ PR)
    # - —Ä—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ —Å force_deploy=true
    # –ù–ï –¥–µ–ø–ª–æ–∏–º –ø—Ä–∏ PR –≤ main (—Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç–∏—Ä—É–µ–º)
    if: |
      always() &&
      (needs.integration-test.result == 'success' || needs.integration-test.result == 'skipped') &&
      (github.event_name == 'push' || github.event.inputs.force_deploy == 'true')
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -p ${{ secrets.SSH_PORT }} ${{ secrets.SSH_HOST }} >> ~/.ssh/known_hosts

      - name: Deploy to server
        env:
          SSH_HOST: ${{ secrets.SSH_HOST }}
          SSH_PORT: ${{ secrets.SSH_PORT }}
          SSH_USERNAME: ${{ secrets.SSH_USERNAME }}
          APP_PATH: ${{ secrets.APP_PATH }}
        run: |
          ssh -p $SSH_PORT $SSH_USERNAME@$SSH_HOST << 'ENDSSH'
            set -e
            cd ${{ secrets.APP_PATH }}
            
            echo "==================================="
            echo "  Production Deployment Started"
            echo "==================================="
            
            echo "üì• Pulling latest changes..."
            git fetch origin
            git reset --hard origin/main
            
            echo "üèóÔ∏è  Building Docker images..."
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–Ω–µ—à–Ω—é—é PostgreSQL (–±–µ–∑ –ø—Ä–æ—Ñ–∏–ª—è postgresql)
            docker compose -f docker/docker-compose.yml build --no-cache
            
            echo "üõë Stopping old containers..."
            docker compose -f docker/docker-compose.yml down
            
            echo "üöÄ Starting new containers..."
            # –ó–∞–ø—É—Å–∫ —Å –≤–Ω–µ—à–Ω–µ–π PostgreSQL (DATABASE_URL –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ .env –∏–ª–∏ secrets)
            docker compose -f docker/docker-compose.yml up -d
            
            echo "üßπ Cleaning up old images..."
            docker image prune -f
            
            echo "‚úÖ Deployment completed successfully!"
          ENDSSH

      - name: Verify deployment
        env:
          SSH_HOST: ${{ secrets.SSH_HOST }}
          SSH_PORT: ${{ secrets.SSH_PORT }}
          SSH_USERNAME: ${{ secrets.SSH_USERNAME }}
          APP_PATH: ${{ secrets.APP_PATH }}
        run: |
          ssh -p $SSH_PORT $SSH_USERNAME@$SSH_HOST << 'ENDSSH'
            cd ${{ secrets.APP_PATH }}
            
            echo "Checking container status..."
            docker compose -f docker/docker-compose.yml ps
            
            echo "Checking backend health..."
            sleep 10
            curl -f http://localhost:3000/api/health || exit 1
            
            echo "All health checks passed!"
          ENDSSH

      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/id_rsa
